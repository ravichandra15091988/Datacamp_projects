{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3df8cd3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 25760 entries, 0 to 25759\n",
      "Data columns (total 10 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   Unnamed: 0       25760 non-null  int64 \n",
      " 1   duration         25760 non-null  object\n",
      " 2   station_A_id     25760 non-null  int64 \n",
      " 3   station_A_name   25760 non-null  object\n",
      " 4   station_B_id     25760 non-null  int64 \n",
      " 5   station_B_name   25760 non-null  object\n",
      " 6   bike_id          25760 non-null  int64 \n",
      " 7   user_type        25760 non-null  int64 \n",
      " 8   user_birth_year  25760 non-null  int64 \n",
      " 9   user_gender      25760 non-null  object\n",
      "dtypes: int64(6), object(4)\n",
      "memory usage: 2.0+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>duration</th>\n",
       "      <th>station_A_id</th>\n",
       "      <th>station_A_name</th>\n",
       "      <th>station_B_id</th>\n",
       "      <th>station_B_name</th>\n",
       "      <th>bike_id</th>\n",
       "      <th>user_type</th>\n",
       "      <th>user_birth_year</th>\n",
       "      <th>user_gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>12 minutes</td>\n",
       "      <td>81</td>\n",
       "      <td>Berry St at 4th St</td>\n",
       "      <td>323</td>\n",
       "      <td>Broadway at Kearny</td>\n",
       "      <td>5480</td>\n",
       "      <td>2</td>\n",
       "      <td>1959</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>24 minutes</td>\n",
       "      <td>3</td>\n",
       "      <td>Powell St BART Station (Market St at 4th St)</td>\n",
       "      <td>118</td>\n",
       "      <td>Eureka Valley Recreation Center</td>\n",
       "      <td>5193</td>\n",
       "      <td>2</td>\n",
       "      <td>1965</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>8 minutes</td>\n",
       "      <td>67</td>\n",
       "      <td>San Francisco Caltrain Station 2  (Townsend St...</td>\n",
       "      <td>23</td>\n",
       "      <td>The Embarcadero at Steuart St</td>\n",
       "      <td>3652</td>\n",
       "      <td>3</td>\n",
       "      <td>1993</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4 minutes</td>\n",
       "      <td>16</td>\n",
       "      <td>Steuart St at Market St</td>\n",
       "      <td>28</td>\n",
       "      <td>The Embarcadero at Bryant St</td>\n",
       "      <td>1883</td>\n",
       "      <td>1</td>\n",
       "      <td>1979</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>11 minutes</td>\n",
       "      <td>22</td>\n",
       "      <td>Howard St at Beale St</td>\n",
       "      <td>350</td>\n",
       "      <td>8th St at Brannan St</td>\n",
       "      <td>4626</td>\n",
       "      <td>2</td>\n",
       "      <td>1994</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0    duration  station_A_id  \\\n",
       "0           0  12 minutes            81   \n",
       "1           1  24 minutes             3   \n",
       "2           2   8 minutes            67   \n",
       "3           3   4 minutes            16   \n",
       "4           4  11 minutes            22   \n",
       "\n",
       "                                      station_A_name  station_B_id  \\\n",
       "0                                 Berry St at 4th St           323   \n",
       "1       Powell St BART Station (Market St at 4th St)           118   \n",
       "2  San Francisco Caltrain Station 2  (Townsend St...            23   \n",
       "3                            Steuart St at Market St            28   \n",
       "4                              Howard St at Beale St           350   \n",
       "\n",
       "                    station_B_name  bike_id  user_type  user_birth_year  \\\n",
       "0               Broadway at Kearny     5480          2             1959   \n",
       "1  Eureka Valley Recreation Center     5193          2             1965   \n",
       "2    The Embarcadero at Steuart St     3652          3             1993   \n",
       "3     The Embarcadero at Bryant St     1883          1             1979   \n",
       "4             8th St at Brannan St     4626          2             1994   \n",
       "\n",
       "  user_gender  \n",
       "0        Male  \n",
       "1        Male  \n",
       "2        Male  \n",
       "3        Male  \n",
       "4        Male  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "ride_sharing = pd.read_csv(\"Downloads/ride_sharing_new.csv\")\n",
    "ride_sharing.info()\n",
    "ride_sharing.head()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1fdfabe2",
   "metadata": {},
   "source": [
    "## describe statistics of u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06df6336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    25760.000000\n",
      "mean         2.008385\n",
      "std          0.704541\n",
      "min          1.000000\n",
      "25%          2.000000\n",
      "50%          2.000000\n",
      "75%          3.000000\n",
      "max          3.000000\n",
      "Name: user_type, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "## Desccribe statistics of user_type column\n",
    "print(ride_sharing['user_type'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04511c71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count     25760\n",
      "unique        3\n",
      "top           2\n",
      "freq      12972\n",
      "Name: user_type_cat, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "## Convert user_type to categorical type\n",
    "ride_sharing['user_type_cat'] = ride_sharing['user_type'].astype('category')\n",
    "\n",
    "## write assert statemnet to confirm the change\n",
    "assert ride_sharing['user_type_cat'].dtype == 'category'\n",
    "\n",
    "print(ride_sharing['user_type_cat'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9acfd4a",
   "metadata": {},
   "source": [
    "## Summing strings and concatenating integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "51cac8dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      duration_trim  duration_time    duration\n",
      "0               12              12  12 minutes\n",
      "1               24              24  24 minutes\n",
      "2                8               8   8 minutes\n",
      "3                4               4   4 minutes\n",
      "4               11              11  11 minutes\n",
      "...             ...            ...         ...\n",
      "25755           11              11  11 minutes\n",
      "25756           10              10  10 minutes\n",
      "25757           14              14  14 minutes\n",
      "25758           14              14  14 minutes\n",
      "25759           29              29  29 minutes\n",
      "\n",
      "[25760 rows x 3 columns]\n",
      "11.389052795031056\n"
     ]
    }
   ],
   "source": [
    "## Strip duration of minutes\n",
    "\n",
    "ride_sharing['duration_trim'] = ride_sharing['duration'].str.strip('minutes')\n",
    "\n",
    "## Convert duration to integer\n",
    "ride_sharing['duration_time'] = ride_sharing['duration_trim'].astype('int')\n",
    "\n",
    "## Write an assert statment to confirm duration_time is an integer\n",
    "\n",
    "assert ride_sharing['duration_time'].dtype == 'int'\n",
    "\n",
    "## print formed columns and calculate average of them\n",
    "\n",
    "print(ride_sharing[['duration_trim', 'duration_time', 'duration']])\n",
    "print(ride_sharing['duration_time'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b72ff7",
   "metadata": {},
   "source": [
    "## Tire size constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "16fbaa74",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'tire_sizes'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3801\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'tire_sizes'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m## Convert tire size to integer\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m ride_sharing[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtire_sizes\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m ride_sharing[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtire_sizes\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mint\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m## set all values of tire sizes above 27 to 27\u001b[39;00m\n\u001b[0;32m      7\u001b[0m ride_sharing\u001b[38;5;241m.\u001b[39mloc[ride_sharing[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtire_sizes\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m27\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtire_sizes\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m27\u001b[39m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:3807\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3805\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3807\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[0;32m   3808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3809\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3804\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3805\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3806\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3808\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3809\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'tire_sizes'"
     ]
    }
   ],
   "source": [
    "## Convert tire size to integer\n",
    "\n",
    "ride_sharing['tire_sizes'] = ride_sharing['tire_sizes'].astype('int')\n",
    "\n",
    "## set all values of tire sizes above 27 to 27\n",
    "\n",
    "ride_sharing.loc[ride_sharing['tire_sizes'] > 27, 'tire_sizes'] = 27\n",
    "\n",
    "## Reconvert tire sizes to categorical\n",
    "\n",
    "ride_sharing['tire_sizes'] = ride_sharing['tire_sizes'].astype('category')\n",
    "\n",
    "ride_sharing['tire_sizes'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147ca78e",
   "metadata": {},
   "source": [
    "# Back to the future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7f9c09bb",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'ride_date'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3801\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'ride_date'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m## Convert ride_dt to datetime\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m ride_sharing[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mride_dt\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(ride_sharing[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mride_date\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m## Save today's date\u001b[39;00m\n\u001b[0;32m      7\u001b[0m today \u001b[38;5;241m=\u001b[39m dt\u001b[38;5;241m.\u001b[39mdate\u001b[38;5;241m.\u001b[39mtoday()\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:3807\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3805\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3807\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[0;32m   3808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3809\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3804\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3805\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3806\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3808\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3809\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'ride_date'"
     ]
    }
   ],
   "source": [
    "## Convert ride_dt to datetime\n",
    "\n",
    "ride_sharing['ride_dt'] = pd.to_datetime(ride_sharing['ride_date'])\n",
    "\n",
    "## Save today's date\n",
    "\n",
    "today = dt.date.today()\n",
    "\n",
    "## Set all in the future dates to today\n",
    "ride_sharing.loc[ride_sharing['ride_dt']>  today, 'ride_dt'] = today\n",
    "\n",
    "## print max of ride_date\n",
    "\n",
    "ride_sharing['ride_dt'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1692dc3",
   "metadata": {},
   "source": [
    "## Finding Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "877b75aa",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "Index(['ride_id'], dtype='object')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Find duplicates\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m duplicated_data \u001b[38;5;241m=\u001b[39m ride_sharing\u001b[38;5;241m.\u001b[39mduplicated(subset \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mride_id\u001b[39m\u001b[38;5;124m'\u001b[39m, keep \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# sort your duplicated data\u001b[39;00m\n\u001b[0;32m      5\u001b[0m sort_duplicate_data \u001b[38;5;241m=\u001b[39m ride_sharing[duplicated_data]\u001b[38;5;241m.\u001b[39msort_values(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mride\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:6806\u001b[0m, in \u001b[0;36mDataFrame.duplicated\u001b[1;34m(self, subset, keep)\u001b[0m\n\u001b[0;32m   6804\u001b[0m diff \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(subset) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[0;32m   6805\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m diff:\n\u001b[1;32m-> 6806\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(Index(diff))\n\u001b[0;32m   6808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(subset) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mis_unique:\n\u001b[0;32m   6809\u001b[0m     \u001b[38;5;66;03m# GH#45236 This is faster than get_group_index below\u001b[39;00m\n\u001b[0;32m   6810\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m[subset[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39mduplicated(keep)\n",
      "\u001b[1;31mKeyError\u001b[0m: Index(['ride_id'], dtype='object')"
     ]
    }
   ],
   "source": [
    "# Find duplicates\n",
    "duplicated_data = ride_sharing.duplicated(subset = 'ride_id', keep = False)\n",
    "\n",
    "# sort your duplicated data\n",
    "sort_duplicate_data = ride_sharing[duplicated_data].sort_values('ride')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ab4f16",
   "metadata": {},
   "source": [
    "## Treating duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "14de4385",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'ride_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 11\u001b[0m\n\u001b[0;32m      7\u001b[0m statistics \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_birth_year\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mduration\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m}\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Group by ride_id and compute statitics\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m ride_unique \u001b[38;5;241m=\u001b[39m ride_sharing\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mride_id\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39magg(statistics)\u001b[38;5;241m.\u001b[39mreset_index()\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:8402\u001b[0m, in \u001b[0;36mDataFrame.groupby\u001b[1;34m(self, by, axis, level, as_index, sort, group_keys, squeeze, observed, dropna)\u001b[0m\n\u001b[0;32m   8399\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have to supply one of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mby\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlevel\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   8400\u001b[0m axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_axis_number(axis)\n\u001b[1;32m-> 8402\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrameGroupBy(\n\u001b[0;32m   8403\u001b[0m     obj\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   8404\u001b[0m     keys\u001b[38;5;241m=\u001b[39mby,\n\u001b[0;32m   8405\u001b[0m     axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[0;32m   8406\u001b[0m     level\u001b[38;5;241m=\u001b[39mlevel,\n\u001b[0;32m   8407\u001b[0m     as_index\u001b[38;5;241m=\u001b[39mas_index,\n\u001b[0;32m   8408\u001b[0m     sort\u001b[38;5;241m=\u001b[39msort,\n\u001b[0;32m   8409\u001b[0m     group_keys\u001b[38;5;241m=\u001b[39mgroup_keys,\n\u001b[0;32m   8410\u001b[0m     squeeze\u001b[38;5;241m=\u001b[39msqueeze,\n\u001b[0;32m   8411\u001b[0m     observed\u001b[38;5;241m=\u001b[39mobserved,\n\u001b[0;32m   8412\u001b[0m     dropna\u001b[38;5;241m=\u001b[39mdropna,\n\u001b[0;32m   8413\u001b[0m )\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py:965\u001b[0m, in \u001b[0;36mGroupBy.__init__\u001b[1;34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, squeeze, observed, mutated, dropna)\u001b[0m\n\u001b[0;32m    962\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m grouper \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    963\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgroupby\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgrouper\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_grouper\n\u001b[1;32m--> 965\u001b[0m     grouper, exclusions, obj \u001b[38;5;241m=\u001b[39m get_grouper(\n\u001b[0;32m    966\u001b[0m         obj,\n\u001b[0;32m    967\u001b[0m         keys,\n\u001b[0;32m    968\u001b[0m         axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[0;32m    969\u001b[0m         level\u001b[38;5;241m=\u001b[39mlevel,\n\u001b[0;32m    970\u001b[0m         sort\u001b[38;5;241m=\u001b[39msort,\n\u001b[0;32m    971\u001b[0m         observed\u001b[38;5;241m=\u001b[39mobserved,\n\u001b[0;32m    972\u001b[0m         mutated\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmutated,\n\u001b[0;32m    973\u001b[0m         dropna\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropna,\n\u001b[0;32m    974\u001b[0m     )\n\u001b[0;32m    976\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj \u001b[38;5;241m=\u001b[39m obj\n\u001b[0;32m    977\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_get_axis_number(axis)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\groupby\\grouper.py:888\u001b[0m, in \u001b[0;36mget_grouper\u001b[1;34m(obj, key, axis, level, sort, observed, mutated, validate, dropna)\u001b[0m\n\u001b[0;32m    886\u001b[0m         in_axis, level, gpr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, gpr, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    887\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 888\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(gpr)\n\u001b[0;32m    889\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(gpr, Grouper) \u001b[38;5;129;01mand\u001b[39;00m gpr\u001b[38;5;241m.\u001b[39mkey \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    890\u001b[0m     \u001b[38;5;66;03m# Add key to exclusions\u001b[39;00m\n\u001b[0;32m    891\u001b[0m     exclusions\u001b[38;5;241m.\u001b[39madd(gpr\u001b[38;5;241m.\u001b[39mkey)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'ride_id'"
     ]
    }
   ],
   "source": [
    "# Drop complete duplicates from ride_sharing\n",
    "\n",
    "ride_dup = ride_sharing.drop_duplicates()\n",
    "\n",
    "# Create dictionary statitics using aggregation function\n",
    "\n",
    "statistics = {'user_birth_year':'min', 'duration':'mean'}\n",
    "\n",
    "# Group by ride_id and compute statitics\n",
    "\n",
    "ride_unique = ride_sharing.groupby('ride_id').agg(statistics).reset_index()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd42a36",
   "metadata": {},
   "source": [
    "# Text and Categorical data problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322701d6",
   "metadata": {},
   "source": [
    "## Find consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1000474f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0    id        day      airline        destination    dest_region  \\\n",
      "0           0  1351    Tuesday  UNITED INTL             KANSAI           Asia   \n",
      "1           1   373     Friday       ALASKA  SAN JOSE DEL CABO  Canada/Mexico   \n",
      "2           2  2820   Thursday        DELTA        LOS ANGELES        West US   \n",
      "3           3  1157    Tuesday    SOUTHWEST        LOS ANGELES        West US   \n",
      "4           4  2992  Wednesday     AMERICAN              MIAMI        East US   \n",
      "\n",
      "  dest_size boarding_area   dept_time  wait_min     cleanliness  \\\n",
      "0       Hub  Gates 91-102  2018-12-31     115.0           Clean   \n",
      "1     Small   Gates 50-59  2018-12-31     135.0           Clean   \n",
      "2       Hub   Gates 40-48  2018-12-31      70.0         Average   \n",
      "3       Hub   Gates 20-39  2018-12-31     190.0           Clean   \n",
      "4       Hub   Gates 50-59  2018-12-31     559.0  Somewhat clean   \n",
      "\n",
      "          safety        satisfaction  \n",
      "0        Neutral      Very satisfied  \n",
      "1      Very safe      Very satisfied  \n",
      "2  Somewhat safe             Neutral  \n",
      "3      Very safe  Somewhat satsified  \n",
      "4      Very safe  Somewhat satsified  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "airline = pd.read_csv(\"Downloads/airlines_final.csv\")\n",
    "\n",
    "print(airline.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e7a7da60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleanliness : ['Clean' 'Average' 'Somewhat clean' 'Somewhat dirty' 'Dirty'] \n",
      "\n",
      "Safety : ['Neutral' 'Very safe' 'Somewhat safe' 'Very unsafe' 'Somewhat unsafe'] \n",
      "\n",
      "Satifaction : ['Very satisfied' 'Neutral' 'Somewhat satsified' 'Somewhat unsatisfied'\n",
      " 'Very unsatisfied'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Cleanliness :', airline['cleanliness'].unique(), \"\\n\")\n",
    "print(\"Safety :\", airline['safety'].unique(), \"\\n\")\n",
    "print(\"Satifaction :\", airline['satisfaction'].unique(), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14ab05f",
   "metadata": {},
   "source": [
    "## Inconsistent Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6fb69940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['asia' 'canada/mexico' 'west us' 'east us' 'midwest us' 'middle east'\n",
      " 'europe' 'central/south america' 'australia/new zealand']\n",
      "['Hub' 'Small' 'Medium' 'Large']\n",
      "['Hub' 'Small' 'Medium' 'Large']\n"
     ]
    }
   ],
   "source": [
    "# print unique values of both cats\n",
    "\n",
    "print(airline['dest_region'].unique())\n",
    "print(airline['dest_size'].unique())\n",
    "\n",
    "## Lower dest_region column and replace \"eur\" with \"europe\"\n",
    "airline['dest_region'] = airline['dest_region'].str.lower()\n",
    "airline['dest_region'] = airline['dest_region'].replace({'eur':'europe'})\n",
    "\n",
    "## Remove dest_size from airlines\n",
    "airline['dest_size'] = airline['dest_size'].str.strip()\n",
    "print(airline['dest_size'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c08832",
   "metadata": {},
   "source": [
    "## Remapping categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1c86ee18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       weekday\n",
       "1       weekday\n",
       "2       weekday\n",
       "3       weekday\n",
       "4       weekday\n",
       "         ...   \n",
       "2472    weekday\n",
       "2473    weekday\n",
       "2474    weekday\n",
       "2475    weekday\n",
       "2476     weeknd\n",
       "Name: day_week, Length: 2477, dtype: object"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Create mappings\n",
    "import numpy as np\n",
    "\n",
    "lable_ranges = [0,60,180,np.inf]\n",
    "lable_names = ['short','medium','long']\n",
    "\n",
    "## Create wait_type column\n",
    "\n",
    "airline['wait_type'] = pd.cut(airline['wait_min'], bins = lable_ranges, labels = lable_names)\n",
    "airline['wait_type']\n",
    "\n",
    "## Create Mappings\n",
    "\n",
    "mappings = {'Monday': 'weekday', 'Tuesday': 'weekday', 'Wednesday': 'weekday', 'Thursday':'weekday', \n",
    "            'Friday':'weekday', \"Saturday\":'weekend', \"Sunday\":'weekend'}\n",
    "\n",
    "airline['day_week'] = airline['day'].replace(mappings)\n",
    "airline['day_week']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "802bdb20",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'full_name'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3801\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'full_name'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[60], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m## Removing Titles and taking names\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m airline[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfull_name\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m airline[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfull_name\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDr.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:3807\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3805\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3807\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[0;32m   3808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3809\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3804\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3805\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3806\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3808\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3809\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'full_name'"
     ]
    }
   ],
   "source": [
    "## Removing Titles and taking names\n",
    "airline['full_name'] = airline['full_name'].str.replace(\"Dr.\", \"  \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c685e199",
   "metadata": {},
   "source": [
    "## Keeping it descriptive\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a8b9b035",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3       Somewhat satsified\n",
       "4       Somewhat satsified\n",
       "6       Somewhat satsified\n",
       "8       Somewhat satsified\n",
       "10      Somewhat satsified\n",
       "               ...        \n",
       "2468    Somewhat satsified\n",
       "2469    Somewhat satsified\n",
       "2471    Somewhat satsified\n",
       "2472    Somewhat satsified\n",
       "2476    Somewhat satsified\n",
       "Name: satisfaction, Length: 1395, dtype: object"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Store length of each row in satisfaction column\n",
    "response_len = airline['satisfaction'].str.len()\n",
    "\n",
    "airline_survey = airline[response_len > 14]\n",
    "\n",
    "## Assert min satisfaction lenght > 14\n",
    "assert airline_survey['satisfaction'].str.len().min() > 14\n",
    "\n",
    "airline_survey['satisfaction']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1045513f",
   "metadata": {},
   "source": [
    "## Advanced Data Problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3bdd9891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100 entries, 0 to 99\n",
      "Data columns (total 12 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   Unnamed: 0        100 non-null    int64  \n",
      " 1   cust_id           100 non-null    object \n",
      " 2   birth_date        100 non-null    object \n",
      " 3   Age               100 non-null    int64  \n",
      " 4   acct_amount       100 non-null    float64\n",
      " 5   inv_amount        100 non-null    int64  \n",
      " 6   fund_A            100 non-null    float64\n",
      " 7   fund_B            100 non-null    float64\n",
      " 8   fund_C            100 non-null    float64\n",
      " 9   fund_D            100 non-null    float64\n",
      " 10  account_opened    100 non-null    object \n",
      " 11  last_transaction  100 non-null    object \n",
      "dtypes: float64(5), int64(3), object(4)\n",
      "memory usage: 9.5+ KB\n"
     ]
    }
   ],
   "source": [
    "## Find values of accu- that are equal to euro\n",
    "\n",
    "bank = pd.read_csv(\"Downloads/banking_dirty.csv\")\n",
    "\n",
    "bank.head()\n",
    "bank.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ff841685",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'accu_cur'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3801\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'accu_cur'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[72], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m acct_eu \u001b[38;5;241m=\u001b[39m bank[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccu_cur\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m euro\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m## Convert accu_amnt to dollars where euro is found\u001b[39;00m\n\u001b[0;32m      5\u001b[0m bank\u001b[38;5;241m.\u001b[39mloc[acct_eu, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccu_amount\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m bank\u001b[38;5;241m.\u001b[39mloc[acct_eu, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccu_amount\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m1.1\u001b[39m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:3807\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3805\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3807\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[0;32m   3808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3809\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3804\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3805\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3806\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3808\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3809\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'accu_cur'"
     ]
    }
   ],
   "source": [
    "acct_eu = bank['accu_cur'] == euro\n",
    "\n",
    "## Convert accu_amnt to dollars where euro is found\n",
    "\n",
    "bank.loc[acct_eu, 'accu_amount'] = bank.loc[acct_eu, 'accu_amount'] *1.1\n",
    "\n",
    "## Unify accu_cur column by changing 'euro' to 'dollars'\n",
    "\n",
    "bank.loc[acct_eu, 'accu_cur'] = 'dollar'\n",
    "\n",
    "# Assert that only dollar currreny remians\n",
    "\n",
    "assert bank['accu_cur'].unique == 'dollar'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a27e3d",
   "metadata": {},
   "source": [
    "## Uniform dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c72f52fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     2018\n",
       "1     2019\n",
       "2     2018\n",
       "3     2017\n",
       "4     2018\n",
       "      ... \n",
       "95    2018\n",
       "96    2017\n",
       "97    2017\n",
       "98    2017\n",
       "99    2017\n",
       "Name: account_year, Length: 100, dtype: object"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bank['account_opened'].head()\n",
    "## Convert account_opened to datetime\n",
    "\n",
    "bank['account_opened'] = pd.to_datetime(bank['account_opened'], infer_datetime_format = True, errors = 'coerce')\n",
    "bank['account_opened']\n",
    "# Get year of account opened\n",
    "bank['account_year'] = bank['account_opened'].dt.strftime('%Y')\n",
    "\n",
    "bank['account_year']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88db9c73",
   "metadata": {},
   "source": [
    "## How's our data integrity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "234d8604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of inconsistent values : 92\n"
     ]
    }
   ],
   "source": [
    "## store fund columns to sum against\n",
    "\n",
    "fund_cols = ['fund_A', 'fund_B', 'fund_C', 'fund_D']\n",
    "\n",
    "## Find rows where fund_cols row sum == inv_amount\n",
    "\n",
    "inv_eqv = bank[fund_cols].sum(axis=1) == bank['inv_amount']\n",
    "inv_eqv\n",
    "\n",
    "## Store consistent and inconsistent inv\n",
    "\n",
    "consistent_inv = bank[inv_eqv]\n",
    "inconsistent_inv = bank[~inv_eqv]\n",
    "\n",
    "## Store consistent and inconsistent data\n",
    "\n",
    "print(\"Number of inconsistent values :\", consistent_inv.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9898ea",
   "metadata": {},
   "source": [
    "## Missing Investors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "fee3667e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0          0\n",
      "cust_id             0\n",
      "birth_date          0\n",
      "Age                 0\n",
      "acct_amount         0\n",
      "inv_amount          0\n",
      "fund_A              0\n",
      "fund_B              0\n",
      "fund_C              0\n",
      "fund_D              0\n",
      "account_opened      0\n",
      "last_transaction    0\n",
      "account_year        0\n",
      "dtype: int64\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'msno' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[101], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(bank\u001b[38;5;241m.\u001b[39misna()\u001b[38;5;241m.\u001b[39msum())\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m## Plot missing \u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m msno\u001b[38;5;241m.\u001b[39mmatrix(bank)\n\u001b[0;32m      7\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m## Isolate missing and non missing values\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'msno' is not defined"
     ]
    }
   ],
   "source": [
    "## print missing values\n",
    "\n",
    "print(bank.isna().sum())\n",
    "\n",
    "## Plot missing \n",
    "msno.matrix(bank)\n",
    "plt.show()\n",
    "\n",
    "## Isolate missing and non missing values\n",
    "missing_investors = bank[bank['inv_amount'].isna()]\n",
    "investors = bank[~bank['inv_amount'].isna()]\n",
    "\n",
    "## Sort bank by age and visualize\n",
    "bank_sort = bank.sort_values('age')\n",
    "masno.matrix(bank_sort)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a05e5c3",
   "metadata": {},
   "source": [
    "# Record Linkage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10cf48b",
   "metadata": {},
   "source": [
    "## The cutoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9270cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "rest = pd.read_csv('Downloads/restaurants_L2.csv')\n",
    "rest.head()\n",
    "\n",
    "from fuzzywuzzy import process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "923f56dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('asian', 100), ('italian', 67), ('american', 62), ('mexican', 50), ('cajun', 40), ('southwestern', 36), ('southern', 31), ('steakhouses', 25), ('coffeebar', 18)]\n",
      "[('american', 100), ('mexican', 80), ('cajun', 68), ('asian', 62), ('italian', 53), ('southern', 38), ('southwestern', 34), ('coffeebar', 24), ('steakhouses', 21)]\n",
      "[('italian', 100), ('asian', 67), ('american', 53), ('mexican', 43), ('cajun', 33), ('southern', 27), ('southwestern', 26), ('steakhouses', 26), ('coffeebar', 12)]\n"
     ]
    }
   ],
   "source": [
    "rest = pd.read_csv('Downloads/restaurants_L2.csv')\n",
    "rest.head()\n",
    "## Store the unique values in cuisine_type\n",
    "\n",
    "unique = rest['type'].unique()\n",
    "unique\n",
    "\n",
    "## Calculate similarity of 'asian' to all unique values of rest\n",
    "print(process.extract('asian', unique, limit= len(unique)))\n",
    "\n",
    "print(process.extract('american', unique, limit= len(unique)))\n",
    "\n",
    "print(process.extract('italian', unique, limit = len(unique)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc67fca2",
   "metadata": {},
   "source": [
    "## Remapping Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9e763e",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "4fc26383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: fuzzywuzzy in c:\\users\\ravi\\appdata\\roaming\\python\\python311\\site-packages (0.18.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install fuzzywuzzy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe10c5ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting python-Levenshtein\n",
      "  Downloading python_Levenshtein-0.21.1-py3-none-any.whl (9.4 kB)\n",
      "Collecting Levenshtein==0.21.1 (from python-Levenshtein)\n",
      "  Downloading Levenshtein-0.21.1-cp311-cp311-win_amd64.whl (101 kB)\n",
      "                                              0.0/101.4 kB ? eta -:--:--\n",
      "     -------------------------------------- 101.4/101.4 kB 2.9 MB/s eta 0:00:00\n",
      "Collecting rapidfuzz<4.0.0,>=2.3.0 (from Levenshtein==0.21.1->python-Levenshtein)\n",
      "  Downloading rapidfuzz-3.1.1-cp311-cp311-win_amd64.whl (1.8 MB)\n",
      "                                              0.0/1.8 MB ? eta -:--:--\n",
      "     ------------                             0.6/1.8 MB 17.5 MB/s eta 0:00:01\n",
      "     ----------------                         0.7/1.8 MB 9.3 MB/s eta 0:00:01\n",
      "     ------------------                       0.9/1.8 MB 7.8 MB/s eta 0:00:01\n",
      "     ------------------                       0.9/1.8 MB 7.8 MB/s eta 0:00:01\n",
      "     ---------------------                    1.0/1.8 MB 4.3 MB/s eta 0:00:01\n",
      "     ----------------------                   1.0/1.8 MB 4.3 MB/s eta 0:00:01\n",
      "     ----------------------                   1.0/1.8 MB 4.3 MB/s eta 0:00:01\n",
      "     -----------------------------            1.3/1.8 MB 3.7 MB/s eta 0:00:01\n",
      "     -------------------------------          1.5/1.8 MB 3.6 MB/s eta 0:00:01\n",
      "     ----------------------------------       1.6/1.8 MB 3.4 MB/s eta 0:00:01\n",
      "     ------------------------------------     1.7/1.8 MB 3.3 MB/s eta 0:00:01\n",
      "     ------------------------------------     1.7/1.8 MB 3.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------  1.8/1.8 MB 3.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.8/1.8 MB 3.0 MB/s eta 0:00:00\n",
      "Installing collected packages: rapidfuzz, Levenshtein, python-Levenshtein\n",
      "Successfully installed Levenshtein-0.21.1 python-Levenshtein-0.21.1 rapidfuzz-3.1.1\n"
     ]
    }
   ],
   "source": [
    "! pip install python-Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44068b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['american' 'asian' 'italian' 'coffeebar' 'southwestern' 'steakhouses'\n",
      " 'southern' 'cajun']\n"
     ]
    }
   ],
   "source": [
    "categories = ['asian', 'american','italian']\n",
    " ## for each correct type in categories\n",
    "\n",
    "for cuisine in categories:\n",
    "    # find cuisine type of resturants\n",
    "    matches = process.extract(cuisine, rest['type'], limit = rest.shape[0])\n",
    "    # for each possible match with similarity score >= 80\n",
    "    for possible_match in matches:\n",
    "        if possible_match[1] >= 80:\n",
    "            # Find matching cuisine type\n",
    "            matching_cuisine = rest['type'] == possible_match[0]\n",
    "            rest.loc[matching_cuisine, 'type'] = cuisine\n",
    "\n",
    "print(rest['type'].unique())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32c23b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting recordlinkage\n",
      "  Downloading recordlinkage-0.16-py3-none-any.whl (926 kB)\n",
      "                                              0.0/926.9 kB ? eta -:--:--\n",
      "     -                                        30.7/926.9 kB ? eta -:--:--\n",
      "     --                                    61.4/926.9 kB 812.7 kB/s eta 0:00:02\n",
      "     ----                                 122.9/926.9 kB 901.1 kB/s eta 0:00:01\n",
      "     --------                               204.8/926.9 kB 1.1 MB/s eta 0:00:01\n",
      "     ----------                             256.0/926.9 kB 1.1 MB/s eta 0:00:01\n",
      "     -------------                          317.4/926.9 kB 1.2 MB/s eta 0:00:01\n",
      "     ----------------                       399.4/926.9 kB 1.2 MB/s eta 0:00:01\n",
      "     --------------------                   491.5/926.9 kB 1.3 MB/s eta 0:00:01\n",
      "     -----------------------                563.2/926.9 kB 1.4 MB/s eta 0:00:01\n",
      "     -------------------------              624.6/926.9 kB 1.4 MB/s eta 0:00:01\n",
      "     ----------------------------           706.6/926.9 kB 1.4 MB/s eta 0:00:01\n",
      "     -------------------------------        768.0/926.9 kB 1.4 MB/s eta 0:00:01\n",
      "     ----------------------------------     829.4/926.9 kB 1.4 MB/s eta 0:00:01\n",
      "     -------------------------------------  921.6/926.9 kB 1.5 MB/s eta 0:00:01\n",
      "     -------------------------------------- 926.9/926.9 kB 1.4 MB/s eta 0:00:00\n",
      "Collecting jellyfish>=1 (from recordlinkage)\n",
      "  Downloading jellyfish-1.0.0-cp311-none-win_amd64.whl (206 kB)\n",
      "                                              0.0/206.5 kB ? eta -:--:--\n",
      "     ---------------                         81.9/206.5 kB 2.3 MB/s eta 0:00:01\n",
      "     ----------------------                 122.9/206.5 kB 1.8 MB/s eta 0:00:01\n",
      "     ------------------------------         163.8/206.5 kB 1.6 MB/s eta 0:00:01\n",
      "     -------------------------------------- 206.5/206.5 kB 1.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.13 in c:\\programdata\\anaconda3\\lib\\site-packages (from recordlinkage) (1.24.3)\n",
      "Requirement already satisfied: pandas<3,>=1 in c:\\programdata\\anaconda3\\lib\\site-packages (from recordlinkage) (1.5.3)\n",
      "Requirement already satisfied: scipy>=1 in c:\\programdata\\anaconda3\\lib\\site-packages (from recordlinkage) (1.10.1)\n",
      "Requirement already satisfied: scikit-learn>=1 in c:\\programdata\\anaconda3\\lib\\site-packages (from recordlinkage) (1.2.2)\n",
      "Requirement already satisfied: joblib in c:\\programdata\\anaconda3\\lib\\site-packages (from recordlinkage) (1.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas<3,>=1->recordlinkage) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas<3,>=1->recordlinkage) (2022.7)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn>=1->recordlinkage) (2.2.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas<3,>=1->recordlinkage) (1.16.0)\n",
      "Installing collected packages: jellyfish, recordlinkage\n",
      "Successfully installed jellyfish-1.0.0 recordlinkage-0.16\n"
     ]
    }
   ],
   "source": [
    "! pip install recordlinkage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec254dd",
   "metadata": {},
   "source": [
    "# Pairs of Restuarants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13c573a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import recordlinkage\n",
    "from recordlinkage.index import Block\n",
    "## Create an indexer and object and find possible pairs\n",
    "indexer = recordlinkage.Index()\n",
    "## Block pairing on cuisine type\n",
    "indexer.block('type')\n",
    "## Generate pairs\n",
    "pairs = indexer.index(rest)\n",
    "pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be839a77",
   "metadata": {},
   "source": [
    "## Similar Restaurants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0653b958",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'label is not found in the dataframe'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m comp_cl\u001b[38;5;241m.\u001b[39mstring(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrest_name\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrest_name\u001b[39m\u001b[38;5;124m'\u001b[39m, label \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m, threshold \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.8\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Get potential matches and print\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m potential_matches \u001b[38;5;241m=\u001b[39m comp_cl\u001b[38;5;241m.\u001b[39mcompute(pairs, rest)\n\u001b[0;32m     12\u001b[0m potential_matches\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\recordlinkage\\base.py:834\u001b[0m, in \u001b[0;36mBaseCompare.compute\u001b[1;34m(self, pairs, x, x_link)\u001b[0m\n\u001b[0;32m    831\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected pandas.DataFrame as third argument\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    833\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 834\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute(pairs, x, x_link)\n\u001b[0;32m    835\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    836\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_parallel(pairs, x, x_link, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\recordlinkage\\base.py:660\u001b[0m, in \u001b[0;36mBaseCompare._compute\u001b[1;34m(self, pairs, x, x_link)\u001b[0m\n\u001b[0;32m    656\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_compute\u001b[39m(\u001b[38;5;28mself\u001b[39m, pairs, x, x_link\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    657\u001b[0m     \u001b[38;5;66;03m# start the timer for the comparing step\u001b[39;00m\n\u001b[0;32m    658\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 660\u001b[0m     sublabels_left \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_labels_left(validate\u001b[38;5;241m=\u001b[39mx)\n\u001b[0;32m    661\u001b[0m     df_a_indexed \u001b[38;5;241m=\u001b[39m frame_indexing(x[sublabels_left], pairs, \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    663\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m x_link \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\recordlinkage\\base.py:628\u001b[0m, in \u001b[0;36mBaseCompare._get_labels_left\u001b[1;34m(self, validate)\u001b[0m\n\u001b[0;32m    626\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_label_dataframe(labels, validate):\n\u001b[0;32m    627\u001b[0m     error_msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel is not found in the dataframe\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 628\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(error_msg)\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m unique(labels)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'label is not found in the dataframe'"
     ]
    }
   ],
   "source": [
    "## Create comparison\n",
    "comp_cl = recordlinkage.Compare()\n",
    "## Find exact matches on city and cuisine_type\n",
    "comp_cl.exact('city','city', label = 'city')\n",
    "comp_cl.exact('type','type', label='type')\n",
    "\n",
    "## Find similar matches of rest_name\n",
    "comp_cl.string('rest_name','rest_name', label = 'name', threshold = 0.8)\n",
    "\n",
    "# Get potential matches and print\n",
    "potential_matches = comp_cl.compute(pairs, rest)\n",
    "potential_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "51a9c3e1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 1 fields in line 31, saw 7\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDownloads/wards_altered.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    947\u001b[0m )\n\u001b[0;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[0;32m    610\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[1;32m--> 611\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\u001b[38;5;241m.\u001b[39mread(nrows)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1778\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1771\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[0;32m   1772\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1773\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m     (\n\u001b[0;32m   1775\u001b[0m         index,\n\u001b[0;32m   1776\u001b[0m         columns,\n\u001b[0;32m   1777\u001b[0m         col_dict,\n\u001b[1;32m-> 1778\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mread(  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m   1779\u001b[0m         nrows\n\u001b[0;32m   1780\u001b[0m     )\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1782\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:230\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[1;32m--> 230\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39mread_low_memory(nrows)\n\u001b[0;32m    231\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    232\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\parsers.pyx:808\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\parsers.pyx:866\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\parsers.pyx:852\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\parsers.pyx:1973\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 1 fields in line 31, saw 7\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('Downloads/wards_altered.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e884f59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
