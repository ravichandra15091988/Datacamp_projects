{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9062fb6c",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88cb1c86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(665, 35)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "volunteer = pd.read_csv('Downloads/volunteer_opportunities.csv')\n",
    "volunteer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f09ea7c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(665, 24)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "volunteer.dropna(axis=1, thresh =3).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a47981d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48\n"
     ]
    }
   ],
   "source": [
    "print(volunteer['category_desc'].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc4f9d88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>opportunity_id</th>\n",
       "      <th>content_id</th>\n",
       "      <th>vol_requests</th>\n",
       "      <th>event_time</th>\n",
       "      <th>title</th>\n",
       "      <th>hits</th>\n",
       "      <th>summary</th>\n",
       "      <th>is_priority</th>\n",
       "      <th>category_id</th>\n",
       "      <th>category_desc</th>\n",
       "      <th>...</th>\n",
       "      <th>end_date_date</th>\n",
       "      <th>status</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Community Board</th>\n",
       "      <th>Community Council</th>\n",
       "      <th>Census Tract</th>\n",
       "      <th>BIN</th>\n",
       "      <th>BBL</th>\n",
       "      <th>NTA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5008</td>\n",
       "      <td>37036</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Web designer</td>\n",
       "      <td>22</td>\n",
       "      <td>Build a website for an Afghan business</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Strengthening Communities</td>\n",
       "      <td>...</td>\n",
       "      <td>February 01 2011</td>\n",
       "      <td>approved</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5016</td>\n",
       "      <td>37143</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>Urban Adventures - Ice Skating at Lasker Rink</td>\n",
       "      <td>62</td>\n",
       "      <td>Please join us and the students from Mott Hall...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Strengthening Communities</td>\n",
       "      <td>...</td>\n",
       "      <td>January 29 2011</td>\n",
       "      <td>approved</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5022</td>\n",
       "      <td>37237</td>\n",
       "      <td>500</td>\n",
       "      <td>0</td>\n",
       "      <td>Fight global hunger and support women farmers ...</td>\n",
       "      <td>14</td>\n",
       "      <td>The Oxfam Action Corps is a group of dedicated...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Strengthening Communities</td>\n",
       "      <td>...</td>\n",
       "      <td>March 31 2012</td>\n",
       "      <td>approved</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5055</td>\n",
       "      <td>37425</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>Stop 'N' Swap</td>\n",
       "      <td>31</td>\n",
       "      <td>Stop 'N' Swap reduces NYC's waste by finding n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Environment</td>\n",
       "      <td>...</td>\n",
       "      <td>February 05 2011</td>\n",
       "      <td>approved</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5056</td>\n",
       "      <td>37426</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>Queens Stop 'N' Swap</td>\n",
       "      <td>135</td>\n",
       "      <td>Stop 'N' Swap reduces NYC's waste by finding n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Environment</td>\n",
       "      <td>...</td>\n",
       "      <td>February 12 2011</td>\n",
       "      <td>approved</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>660</th>\n",
       "      <td>5640</td>\n",
       "      <td>50193</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Volunteer for NYLAG's Food Stamps Project</td>\n",
       "      <td>197</td>\n",
       "      <td>Volunteers needed to file for fair hearings, d...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Helping Neighbors in Need</td>\n",
       "      <td>...</td>\n",
       "      <td>November 15 2012</td>\n",
       "      <td>approved</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>661</th>\n",
       "      <td>5218</td>\n",
       "      <td>38711</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>Iridescent Science Studio Open House Volunteers</td>\n",
       "      <td>113</td>\n",
       "      <td>Come out to the South Bronx to help us hold ou...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Strengthening Communities</td>\n",
       "      <td>...</td>\n",
       "      <td>April 13 2011</td>\n",
       "      <td>approved</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>662</th>\n",
       "      <td>5541</td>\n",
       "      <td>47820</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>French Translator</td>\n",
       "      <td>145</td>\n",
       "      <td>Volunteer needed to translate written material...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Helping Neighbors in Need</td>\n",
       "      <td>...</td>\n",
       "      <td>September 01 2011</td>\n",
       "      <td>approved</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>663</th>\n",
       "      <td>5398</td>\n",
       "      <td>40722</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Marketing &amp; Advertising Volunteer</td>\n",
       "      <td>330</td>\n",
       "      <td>World Cares Center is looking for individuals ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Strengthening Communities</td>\n",
       "      <td>...</td>\n",
       "      <td>May 31 2012</td>\n",
       "      <td>approved</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>664</th>\n",
       "      <td>5507</td>\n",
       "      <td>44303</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>Volunteer filmmakers to help Mayor's Office wi...</td>\n",
       "      <td>304</td>\n",
       "      <td>Attention all filmmakers, producers, and edito...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Strengthening Communities</td>\n",
       "      <td>...</td>\n",
       "      <td>October 06 2012</td>\n",
       "      <td>approved</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>617 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     opportunity_id  content_id  vol_requests  event_time  \\\n",
       "1              5008       37036             2           0   \n",
       "2              5016       37143            20           0   \n",
       "3              5022       37237           500           0   \n",
       "4              5055       37425            15           0   \n",
       "5              5056       37426            15           0   \n",
       "..              ...         ...           ...         ...   \n",
       "660            5640       50193             3           0   \n",
       "661            5218       38711            10           0   \n",
       "662            5541       47820             1           0   \n",
       "663            5398       40722             2           0   \n",
       "664            5507       44303             5           0   \n",
       "\n",
       "                                                 title  hits  \\\n",
       "1                                         Web designer    22   \n",
       "2        Urban Adventures - Ice Skating at Lasker Rink    62   \n",
       "3    Fight global hunger and support women farmers ...    14   \n",
       "4                                        Stop 'N' Swap    31   \n",
       "5                                 Queens Stop 'N' Swap   135   \n",
       "..                                                 ...   ...   \n",
       "660          Volunteer for NYLAG's Food Stamps Project   197   \n",
       "661    Iridescent Science Studio Open House Volunteers   113   \n",
       "662                                  French Translator   145   \n",
       "663                  Marketing & Advertising Volunteer   330   \n",
       "664  Volunteer filmmakers to help Mayor's Office wi...   304   \n",
       "\n",
       "                                               summary is_priority  \\\n",
       "1               Build a website for an Afghan business         NaN   \n",
       "2    Please join us and the students from Mott Hall...         NaN   \n",
       "3    The Oxfam Action Corps is a group of dedicated...         NaN   \n",
       "4    Stop 'N' Swap reduces NYC's waste by finding n...         NaN   \n",
       "5    Stop 'N' Swap reduces NYC's waste by finding n...         NaN   \n",
       "..                                                 ...         ...   \n",
       "660  Volunteers needed to file for fair hearings, d...         NaN   \n",
       "661  Come out to the South Bronx to help us hold ou...         NaN   \n",
       "662  Volunteer needed to translate written material...         NaN   \n",
       "663  World Cares Center is looking for individuals ...         NaN   \n",
       "664  Attention all filmmakers, producers, and edito...         NaN   \n",
       "\n",
       "     category_id              category_desc  ...      end_date_date    status  \\\n",
       "1            1.0  Strengthening Communities  ...   February 01 2011  approved   \n",
       "2            1.0  Strengthening Communities  ...    January 29 2011  approved   \n",
       "3            1.0  Strengthening Communities  ...      March 31 2012  approved   \n",
       "4            4.0                Environment  ...   February 05 2011  approved   \n",
       "5            4.0                Environment  ...   February 12 2011  approved   \n",
       "..           ...                        ...  ...                ...       ...   \n",
       "660          2.0  Helping Neighbors in Need  ...   November 15 2012  approved   \n",
       "661          1.0  Strengthening Communities  ...      April 13 2011  approved   \n",
       "662          2.0  Helping Neighbors in Need  ...  September 01 2011  approved   \n",
       "663          1.0  Strengthening Communities  ...        May 31 2012  approved   \n",
       "664          1.0  Strengthening Communities  ...    October 06 2012  approved   \n",
       "\n",
       "    Latitude  Longitude  Community Board Community Council  Census Tract  BIN  \\\n",
       "1        NaN        NaN              NaN                NaN          NaN  NaN   \n",
       "2        NaN        NaN              NaN                NaN          NaN  NaN   \n",
       "3        NaN        NaN              NaN                NaN          NaN  NaN   \n",
       "4        NaN        NaN              NaN                NaN          NaN  NaN   \n",
       "5        NaN        NaN              NaN                NaN          NaN  NaN   \n",
       "..       ...        ...              ...                ...          ...  ...   \n",
       "660      NaN        NaN              NaN                NaN          NaN  NaN   \n",
       "661      NaN        NaN              NaN                NaN          NaN  NaN   \n",
       "662      NaN        NaN              NaN                NaN          NaN  NaN   \n",
       "663      NaN        NaN              NaN                NaN          NaN  NaN   \n",
       "664      NaN        NaN              NaN                NaN          NaN  NaN   \n",
       "\n",
       "     BBL NTA  \n",
       "1    NaN NaN  \n",
       "2    NaN NaN  \n",
       "3    NaN NaN  \n",
       "4    NaN NaN  \n",
       "5    NaN NaN  \n",
       "..   ...  ..  \n",
       "660  NaN NaN  \n",
       "661  NaN NaN  \n",
       "662  NaN NaN  \n",
       "663  NaN NaN  \n",
       "664  NaN NaN  \n",
       "\n",
       "[617 rows x 35 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "volunteer_subset = volunteer[volunteer['category_desc'].notnull()]\n",
    "volunteer_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "243d51d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{dtype('int64'), dtype('float64'), dtype('O')}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(volunteer.dtypes.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1326859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 665 entries, 0 to 664\n",
      "Data columns (total 35 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   opportunity_id      665 non-null    int64  \n",
      " 1   content_id          665 non-null    int64  \n",
      " 2   vol_requests        665 non-null    int64  \n",
      " 3   event_time          665 non-null    int64  \n",
      " 4   title               665 non-null    object \n",
      " 5   hits                665 non-null    int64  \n",
      " 6   summary             665 non-null    object \n",
      " 7   is_priority         62 non-null     object \n",
      " 8   category_id         617 non-null    float64\n",
      " 9   category_desc       617 non-null    object \n",
      " 10  amsl                0 non-null      float64\n",
      " 11  amsl_unit           0 non-null      float64\n",
      " 12  org_title           665 non-null    object \n",
      " 13  org_content_id      665 non-null    int64  \n",
      " 14  addresses_count     665 non-null    int64  \n",
      " 15  locality            595 non-null    object \n",
      " 16  region              665 non-null    object \n",
      " 17  postalcode          659 non-null    float64\n",
      " 18  primary_loc         0 non-null      float64\n",
      " 19  display_url         665 non-null    object \n",
      " 20  recurrence_type     665 non-null    object \n",
      " 21  hours               665 non-null    int64  \n",
      " 22  created_date        665 non-null    object \n",
      " 23  last_modified_date  665 non-null    object \n",
      " 24  start_date_date     665 non-null    object \n",
      " 25  end_date_date       665 non-null    object \n",
      " 26  status              665 non-null    object \n",
      " 27  Latitude            0 non-null      float64\n",
      " 28  Longitude           0 non-null      float64\n",
      " 29  Community Board     0 non-null      float64\n",
      " 30  Community Council   0 non-null      float64\n",
      " 31  Census Tract        0 non-null      float64\n",
      " 32  BIN                 0 non-null      float64\n",
      " 33  BBL                 0 non-null      float64\n",
      " 34  NTA                 0 non-null      float64\n",
      "dtypes: float64(13), int64(8), object(14)\n",
      "memory usage: 182.0+ KB\n"
     ]
    }
   ],
   "source": [
    "volunteer.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0e53c4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('int64')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "volunteer['hits'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2bb82136",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('O')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "volunteer['last_modified_date'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d86a3336",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Strengthening Communities    307\n",
       "Helping Neighbors in Need    119\n",
       "Education                     92\n",
       "Health                        52\n",
       "Environment                   32\n",
       "Emergency Preparedness        15\n",
       "Name: category_desc, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "volunteer_subset.category_desc.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9631324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strengthening Communities    230\n",
      "Helping Neighbors in Need     89\n",
      "Education                     69\n",
      "Health                        39\n",
      "Environment                   24\n",
      "Emergency Preparedness        11\n",
      "Name: category_desc, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Create a data with all columns except category_desc\n",
    "volunteer_X = volunteer_subset.drop('category_desc', axis = 1)\n",
    "# Create a separate dataset including category_desc\n",
    "volunteer_y = volunteer_subset[['category_desc']]\n",
    "# Use stratified sampling to split the dataset accoring to the volunteer_y dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(volunteer_X, volunteer_y, stratify = volunteer_y)\n",
    "\n",
    "\n",
    "# Print out the category_desc counts on the training y labels\n",
    "print(y_train['category_desc'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b70d1da",
   "metadata": {},
   "source": [
    "# Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "85fb4c47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>Malic acid</th>\n",
       "      <th>Ash</th>\n",
       "      <th>Alcalinity of ash</th>\n",
       "      <th>Magnesium</th>\n",
       "      <th>Total phenols</th>\n",
       "      <th>Flavanoids</th>\n",
       "      <th>Nonflavanoid phenols</th>\n",
       "      <th>Proanthocyanins</th>\n",
       "      <th>Color intensity</th>\n",
       "      <th>Hue</th>\n",
       "      <th>OD280/OD315 of diluted wines</th>\n",
       "      <th>Proline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Type  Alcohol  Malic acid   Ash  Alcalinity of ash  Magnesium  \\\n",
       "0     1    14.23        1.71  2.43               15.6        127   \n",
       "1     1    13.20        1.78  2.14               11.2        100   \n",
       "2     1    13.16        2.36  2.67               18.6        101   \n",
       "3     1    14.37        1.95  2.50               16.8        113   \n",
       "4     1    13.24        2.59  2.87               21.0        118   \n",
       "\n",
       "   Total phenols  Flavanoids  Nonflavanoid phenols  Proanthocyanins  \\\n",
       "0           2.80        3.06                  0.28             2.29   \n",
       "1           2.65        2.76                  0.26             1.28   \n",
       "2           2.80        3.24                  0.30             2.81   \n",
       "3           3.85        3.49                  0.24             2.18   \n",
       "4           2.80        2.69                  0.39             1.82   \n",
       "\n",
       "   Color intensity   Hue  OD280/OD315 of diluted wines  Proline  \n",
       "0             5.64  1.04                          3.92     1065  \n",
       "1             4.38  1.05                          3.40     1050  \n",
       "2             5.68  1.03                          3.17     1185  \n",
       "3             7.80  0.86                          3.45     1480  \n",
       "4             4.32  1.04                          2.93      735  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine = pd.read_csv('Downloads/wine_types.csv')\n",
    "wine.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ec283799",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Type                                0.600679\n",
       "Alcohol                             0.659062\n",
       "Malic acid                          1.248015\n",
       "Ash                                 0.075265\n",
       "Alcalinity of ash                  11.152686\n",
       "Magnesium                         203.989335\n",
       "Total phenols                       0.391690\n",
       "Flavanoids                          0.997719\n",
       "Nonflavanoid phenols                0.015489\n",
       "Proanthocyanins                     0.327595\n",
       "Color intensity                     5.374449\n",
       "Hue                                 0.052245\n",
       "OD280/OD315 of diluted wines        0.504086\n",
       "Proline                         99166.717355\n",
       "dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5e711715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17231366191842018\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "wine['Proline_log'] = np.log(wine.Proline)\n",
    "print(wine.Proline_log.var())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fd9670",
   "metadata": {},
   "source": [
    "## Scaling data for feature comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "41d3782f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Alcalinity of ash</th>\n",
       "      <th>Ash</th>\n",
       "      <th>Magnesium</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>19.494944</td>\n",
       "      <td>2.366517</td>\n",
       "      <td>99.741573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.339564</td>\n",
       "      <td>0.274344</td>\n",
       "      <td>14.282484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>10.600000</td>\n",
       "      <td>1.360000</td>\n",
       "      <td>70.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>17.200000</td>\n",
       "      <td>2.210000</td>\n",
       "      <td>88.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>19.500000</td>\n",
       "      <td>2.360000</td>\n",
       "      <td>98.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>21.500000</td>\n",
       "      <td>2.557500</td>\n",
       "      <td>107.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>30.000000</td>\n",
       "      <td>3.230000</td>\n",
       "      <td>162.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Alcalinity of ash         Ash   Magnesium\n",
       "count         178.000000  178.000000  178.000000\n",
       "mean           19.494944    2.366517   99.741573\n",
       "std             3.339564    0.274344   14.282484\n",
       "min            10.600000    1.360000   70.000000\n",
       "25%            17.200000    2.210000   88.000000\n",
       "50%            19.500000    2.360000   98.000000\n",
       "75%            21.500000    2.557500  107.000000\n",
       "max            30.000000    3.230000  162.000000"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine[['Alcalinity of ash','Ash','Magnesium']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "527c2ad6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.16959318,  0.23205254,  1.91390522],\n",
       "       [-2.49084714, -0.82799632,  0.01814502],\n",
       "       [-0.2687382 ,  1.10933436,  0.08835836],\n",
       "       [-0.80925118,  0.4879264 ,  0.93091845],\n",
       "       [ 0.45194578,  1.84040254,  1.28198515]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import StandardScaler from scikit-learn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "ss = StandardScaler()\n",
    "wine_subset = wine[['Alcalinity of ash','Ash','Magnesium']]\n",
    "wine_subset_scaled = ss.fit_transform(wine_subset)\n",
    "wine_subset_scaled[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f873728b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.022222222222222223"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "X = wine.drop('Proline', axis =1)\n",
    "y = wine['Proline']\n",
    "X_train,X_test, y_train, y_test = train_test_split(X,y)\n",
    "model = KNeighborsClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b670499f",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1651d9",
   "metadata": {},
   "source": [
    "### Encoding categorical variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb472ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "hiking = pd.read_json('Downloads/hiking.json')\n",
    "hiking.info()\n",
    "enc =LabelEncoder()\n",
    "hiking['Accessible'].value_counts()\n",
    "\n",
    "hiking['Accessible_enc'] = enc.fit_transform(hiking.Accessible)\n",
    "print(hiking[['Accessible','Accessible_enc']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d31d3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "category_enc = pd.get_dummies(volunteer['category_desc'])\n",
    "print(category_enc.head())\n",
    "\n",
    "enc = OneHotEncoder()\n",
    "enc_data = pd.DataFrame(enc.fit_transform(category_enc).toarray())\n",
    "\n",
    "volunteer_new = volunteer_subset.join(enc_data)\n",
    "volunteer_new.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61f8fdc",
   "metadata": {},
   "source": [
    "## Engineering numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b424c5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Crate a list of columns to average\n",
    "run_columns = ['run1', 'run2', 'run3', 'run4', 'run5']\n",
    "running_times_5k['mean'] = running_times_5k.apply(lambda row: row[run_columns].mean(),axis = 1)\n",
    "print(running_times_5k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a32fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Datetime\n",
    "volunteer['datetime_converted'] = pd.to_datetime(volunteer['start_date_date'])\n",
    "# Extract just the month from the converted column\n",
    "volunteer['start_date_month'] = volunteer['datetime_converted'].apply(lambda row: row.month)\n",
    "\n",
    "print(volunteer[['start_date_month','datetime_converted']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60ca17b",
   "metadata": {},
   "source": [
    "## Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6387aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "hiking['Length'] = hiking['Length'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc8da6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Engineering features from strings:Extraction\n",
    "# Write a pattern to extract numbers and decimals\n",
    "def return_mileage(length):\n",
    "    pattern = re.compile(r\"\\d+\\.\\d+\")\n",
    "     \n",
    "    # Search the text for matches\n",
    "    mile = re.match(pattern, length)\n",
    "     \n",
    "    # If a value is returned, use group(0) to return the found value\n",
    "    if mile is not None:\n",
    "        return float(mile.group(0))\n",
    "         \n",
    "# Apply the function to the Length column and take a look at both columns\n",
    "hiking[\"Length_num\"] = hiking['Length'].apply(lambda row: return_mileage(row))\n",
    "print(hiking[[\"Length\", \"Length_num\"]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "773759e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Engineering features from strings:tf/idf\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "title_text = volunteer_subset['title']\n",
    "tfidf_vec = TfidfVectorizer()\n",
    "text_tfidf = tfidf_vec.fit_transform(title_text)\n",
    "text_tfidf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "15a496ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6\n"
     ]
    }
   ],
   "source": [
    "## Text classification using tf/idf vectors\n",
    "text_tfidf.toarray().shape\n",
    "y = volunteer_subset['category_desc']\n",
    "X_train,X_test,y_train,y_test = train_test_split(text_tfidf.toarray(), y, stratify = y)\n",
    "\n",
    "# Fit the Naive bayes model to the train_data\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "model = GaussianNB()\n",
    "model.fit(X_train, y_train)\n",
    "print(model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c892e84",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0153c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "volunteer.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cb7c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = [\"locality\", \"region\", \"category_desc\", \"created_date\", \"vol_requests\"]\n",
    "volunteer_subset = volunteer.drop(to_drop, axis = 1)\n",
    "volunteer_subset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697332b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Using correlation matrix\n",
    "wine.corr()\n",
    "to_drop = 'Flavanoids'\n",
    "wine_subset = wine.drop(to_drop, axis=1)\n",
    "wine_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5881727",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a32f1bee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<617x1089 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 3172 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = {1048: 'web', 278: 'designer', 1017: 'urban'}\n",
    " \n",
    "tfidf_vec = TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
    "         encoding='utf-8', input='content',\n",
    "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
    "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
    "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
    "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
    "        vocabulary=None)\n",
    " \n",
    "tfidf_vec.vocabulary_ = {'web': 1048, 'designer': 278, 'urban': 1017}\n",
    " \n",
    "text_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f7fe1255",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "409",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m     zipped_index \u001b[38;5;241m=\u001b[39m zipped_series\u001b[38;5;241m.\u001b[39msort_values(ascending \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m)[:top_n]\u001b[38;5;241m.\u001b[39mindex\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [original_vocab[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m zipped_index]\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28mprint\u001b[39m(return_weights(vocab, tfidf_vec\u001b[38;5;241m.\u001b[39mvocabulary_, text_tfidf, vector_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, top_n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m))\n",
      "Cell \u001b[1;32mIn[39], line 7\u001b[0m, in \u001b[0;36mreturn_weights\u001b[1;34m(vocab, original_vocab, vector, vector_index, top_n)\u001b[0m\n\u001b[0;32m      4\u001b[0m zipped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(vector[vector_index]\u001b[38;5;241m.\u001b[39mindices, vector[vector_index]\u001b[38;5;241m.\u001b[39mdata))\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Let's transform that zipped dict into a series\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m zipped_series \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mSeries({vocab[i]:zipped[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m vector[vector_index]\u001b[38;5;241m.\u001b[39mindices})\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Let's sort the series to pull out the top n weighted words\u001b[39;00m\n\u001b[0;32m     10\u001b[0m zipped_index \u001b[38;5;241m=\u001b[39m zipped_series\u001b[38;5;241m.\u001b[39msort_values(ascending \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m)[:top_n]\u001b[38;5;241m.\u001b[39mindex\n",
      "Cell \u001b[1;32mIn[39], line 7\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      4\u001b[0m zipped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(vector[vector_index]\u001b[38;5;241m.\u001b[39mindices, vector[vector_index]\u001b[38;5;241m.\u001b[39mdata))\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Let's transform that zipped dict into a series\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m zipped_series \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mSeries({vocab[i]:zipped[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m vector[vector_index]\u001b[38;5;241m.\u001b[39mindices})\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Let's sort the series to pull out the top n weighted words\u001b[39;00m\n\u001b[0;32m     10\u001b[0m zipped_index \u001b[38;5;241m=\u001b[39m zipped_series\u001b[38;5;241m.\u001b[39msort_values(ascending \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m)[:top_n]\u001b[38;5;241m.\u001b[39mindex\n",
      "\u001b[1;31mKeyError\u001b[0m: 409"
     ]
    }
   ],
   "source": [
    "# Exploring text vectors, part 1\n",
    "\n",
    "def return_weights(vocab, original_vocab, vector, vector_index, top_n):\n",
    "    zipped = dict(zip(vector[vector_index].indices, vector[vector_index].data))\n",
    "    \n",
    "    # Let's transform that zipped dict into a series\n",
    "    zipped_series = pd.Series({vocab[i]:zipped[i] for i in vector[vector_index].indices})\n",
    "    \n",
    "    # Let's sort the series to pull out the top n weighted words\n",
    "    zipped_index = zipped_series.sort_values(ascending = False)[:top_n].index\n",
    "    return [original_vocab[i] for i in zipped_index]\n",
    "\n",
    "print(return_weights(vocab, tfidf_vec.vocabulary_, text_tfidf, vector_index=8, top_n=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7babdeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "82731747",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "832",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mset\u001b[39m(filter_list)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Call the function to get the list of word indices\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m filtered_words \u001b[38;5;241m=\u001b[39m words_to_filter(vocab, tfidf_vec\u001b[38;5;241m.\u001b[39mvocabulary_, text_tfidf, \u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# By converting filtered_words back to a list, we can use it to filter the columns in the text vector\u001b[39;00m\n\u001b[0;32m     15\u001b[0m filtered_text \u001b[38;5;241m=\u001b[39m text_tfidf[:, \u001b[38;5;28mlist\u001b[39m(filtered_words)]\n",
      "Cell \u001b[1;32mIn[40], line 6\u001b[0m, in \u001b[0;36mwords_to_filter\u001b[1;34m(vocab, original_vocab, vector, top_n)\u001b[0m\n\u001b[0;32m      2\u001b[0m filter_list \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, vector\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]):\n\u001b[0;32m      4\u001b[0m  \n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# Here we'll call the function from the previous exercise, and extend the list we're creating\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m     filtered \u001b[38;5;241m=\u001b[39m return_weights(vocab, original_vocab, vector, i, top_n)\n\u001b[0;32m      7\u001b[0m     filter_list\u001b[38;5;241m.\u001b[39mextend(filtered)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Return the list in a set, so we don't get duplicate word indices\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[39], line 7\u001b[0m, in \u001b[0;36mreturn_weights\u001b[1;34m(vocab, original_vocab, vector, vector_index, top_n)\u001b[0m\n\u001b[0;32m      4\u001b[0m zipped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(vector[vector_index]\u001b[38;5;241m.\u001b[39mindices, vector[vector_index]\u001b[38;5;241m.\u001b[39mdata))\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Let's transform that zipped dict into a series\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m zipped_series \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mSeries({vocab[i]:zipped[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m vector[vector_index]\u001b[38;5;241m.\u001b[39mindices})\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Let's sort the series to pull out the top n weighted words\u001b[39;00m\n\u001b[0;32m     10\u001b[0m zipped_index \u001b[38;5;241m=\u001b[39m zipped_series\u001b[38;5;241m.\u001b[39msort_values(ascending \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m)[:top_n]\u001b[38;5;241m.\u001b[39mindex\n",
      "Cell \u001b[1;32mIn[39], line 7\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      4\u001b[0m zipped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(vector[vector_index]\u001b[38;5;241m.\u001b[39mindices, vector[vector_index]\u001b[38;5;241m.\u001b[39mdata))\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Let's transform that zipped dict into a series\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m zipped_series \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mSeries({vocab[i]:zipped[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m vector[vector_index]\u001b[38;5;241m.\u001b[39mindices})\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Let's sort the series to pull out the top n weighted words\u001b[39;00m\n\u001b[0;32m     10\u001b[0m zipped_index \u001b[38;5;241m=\u001b[39m zipped_series\u001b[38;5;241m.\u001b[39msort_values(ascending \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m)[:top_n]\u001b[38;5;241m.\u001b[39mindex\n",
      "\u001b[1;31mKeyError\u001b[0m: 832"
     ]
    }
   ],
   "source": [
    "def words_to_filter(vocab, original_vocab, vector, top_n):\n",
    "    filter_list = []\n",
    "    for i in range(0, vector.shape[0]):\n",
    "     \n",
    "        # Here we'll call the function from the previous exercise, and extend the list we're creating\n",
    "        filtered = return_weights(vocab, original_vocab, vector, i, top_n)\n",
    "        filter_list.extend(filtered)\n",
    "    # Return the list in a set, so we don't get duplicate word indices\n",
    "    return set(filter_list)\n",
    " \n",
    "# Call the function to get the list of word indices\n",
    "filtered_words = words_to_filter(vocab, tfidf_vec.vocabulary_, text_tfidf, 3)\n",
    " \n",
    "# By converting filtered_words back to a list, we can use it to filter the columns in the text vector\n",
    "filtered_text = text_tfidf[:, list(filtered_words)]\n",
    " \n",
    "filtered_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd30360",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f680687b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9.98091230e-01 1.73591562e-03 9.49589576e-05 5.02173562e-05\n",
      " 1.23636847e-05 8.46213034e-06 2.80681456e-06 1.52308053e-06\n",
      " 1.12783044e-06 7.21415811e-07 3.78060267e-07 2.12013755e-07\n",
      " 8.25392788e-08]\n"
     ]
    }
   ],
   "source": [
    "## Using PCA\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA()\n",
    "\n",
    "wine_X = wine.drop('Type', axis = 1)\n",
    "\n",
    "transformed_X = pca.fit_transform(wine_X)\n",
    "\n",
    "# Look at the percentage of variance explained by the different components\n",
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b049f35c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17777777777777778"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# Training a model with PCA\n",
    "wine_X_train, wine_X_test, wine_y_train, wine_y_test = train_test_split(transformed_X, y)\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(wine_X_train, wine_y_train)\n",
    "knn.score(wine_X_test, wine_y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e536daba",
   "metadata": {},
   "source": [
    "# Putting it altogether"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3b865a07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>country</th>\n",
       "      <th>type</th>\n",
       "      <th>seconds</th>\n",
       "      <th>length_of_time</th>\n",
       "      <th>desc</th>\n",
       "      <th>recorded</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11/3/2011 19:21</td>\n",
       "      <td>woodville</td>\n",
       "      <td>wi</td>\n",
       "      <td>us</td>\n",
       "      <td>unknown</td>\n",
       "      <td>1209600.0</td>\n",
       "      <td>2 weeks</td>\n",
       "      <td>Red blinking objects similar to airplanes or s...</td>\n",
       "      <td>12/12/2011</td>\n",
       "      <td>44.9530556</td>\n",
       "      <td>-92.291111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10/3/2004 19:05</td>\n",
       "      <td>cleveland</td>\n",
       "      <td>oh</td>\n",
       "      <td>us</td>\n",
       "      <td>circle</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30sec.</td>\n",
       "      <td>Many fighter jets flying towards UFO</td>\n",
       "      <td>10/27/2004</td>\n",
       "      <td>41.4994444</td>\n",
       "      <td>-81.695556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9/25/2009 21:00</td>\n",
       "      <td>coon rapids</td>\n",
       "      <td>mn</td>\n",
       "      <td>us</td>\n",
       "      <td>cigar</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Green&amp;#44 red&amp;#44 and blue pulses of light tha...</td>\n",
       "      <td>12/12/2009</td>\n",
       "      <td>45.1200000</td>\n",
       "      <td>-93.287500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11/21/2002 05:45</td>\n",
       "      <td>clemmons</td>\n",
       "      <td>nc</td>\n",
       "      <td>us</td>\n",
       "      <td>triangle</td>\n",
       "      <td>300.0</td>\n",
       "      <td>about 5 minutes</td>\n",
       "      <td>It was a large&amp;#44 triangular shaped flying ob...</td>\n",
       "      <td>12/23/2002</td>\n",
       "      <td>36.0213889</td>\n",
       "      <td>-80.382222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8/19/2010 12:55</td>\n",
       "      <td>calgary (canada)</td>\n",
       "      <td>ab</td>\n",
       "      <td>ca</td>\n",
       "      <td>oval</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>A white spinning disc in the shape of an oval.</td>\n",
       "      <td>8/24/2010</td>\n",
       "      <td>51.083333</td>\n",
       "      <td>-114.083333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               date              city state country      type    seconds  \\\n",
       "0   11/3/2011 19:21         woodville    wi      us   unknown  1209600.0   \n",
       "1   10/3/2004 19:05         cleveland    oh      us    circle       30.0   \n",
       "2   9/25/2009 21:00       coon rapids    mn      us     cigar        0.0   \n",
       "3  11/21/2002 05:45          clemmons    nc      us  triangle      300.0   \n",
       "4   8/19/2010 12:55  calgary (canada)    ab      ca      oval        0.0   \n",
       "\n",
       "    length_of_time                                               desc  \\\n",
       "0          2 weeks  Red blinking objects similar to airplanes or s...   \n",
       "1           30sec.               Many fighter jets flying towards UFO   \n",
       "2              NaN  Green&#44 red&#44 and blue pulses of light tha...   \n",
       "3  about 5 minutes  It was a large&#44 triangular shaped flying ob...   \n",
       "4                2     A white spinning disc in the shape of an oval.   \n",
       "\n",
       "     recorded         lat        long  \n",
       "0  12/12/2011  44.9530556  -92.291111  \n",
       "1  10/27/2004  41.4994444  -81.695556  \n",
       "2  12/12/2009  45.1200000  -93.287500  \n",
       "3  12/23/2002  36.0213889  -80.382222  \n",
       "4   8/24/2010   51.083333 -114.083333  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "ufo = pd.read_csv('Downloads/ufo_sightings_large.csv')\n",
    "ufo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "55287906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4935 entries, 0 to 4934\n",
      "Data columns (total 11 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   date            4935 non-null   object \n",
      " 1   city            4926 non-null   object \n",
      " 2   state           4516 non-null   object \n",
      " 3   country         4255 non-null   object \n",
      " 4   type            4776 non-null   object \n",
      " 5   seconds         4935 non-null   float64\n",
      " 6   length_of_time  4792 non-null   object \n",
      " 7   desc            4932 non-null   object \n",
      " 8   recorded        4935 non-null   object \n",
      " 9   lat             4935 non-null   object \n",
      " 10  long            4935 non-null   float64\n",
      "dtypes: float64(2), object(9)\n",
      "memory usage: 424.2+ KB\n"
     ]
    }
   ],
   "source": [
    "ufo.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6b66511a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datetime64[ns]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4935 entries, 0 to 4934\n",
      "Data columns (total 11 columns):\n",
      " #   Column          Non-Null Count  Dtype         \n",
      "---  ------          --------------  -----         \n",
      " 0   date            4935 non-null   datetime64[ns]\n",
      " 1   city            4926 non-null   object        \n",
      " 2   state           4516 non-null   object        \n",
      " 3   country         4255 non-null   object        \n",
      " 4   type            4776 non-null   object        \n",
      " 5   seconds         4935 non-null   float64       \n",
      " 6   length_of_time  4792 non-null   object        \n",
      " 7   desc            4932 non-null   object        \n",
      " 8   recorded        4935 non-null   object        \n",
      " 9   lat             4935 non-null   object        \n",
      " 10  long            4935 non-null   float64       \n",
      "dtypes: datetime64[ns](1), float64(2), object(8)\n",
      "memory usage: 424.2+ KB\n"
     ]
    }
   ],
   "source": [
    "ufo['date'] = pd.to_datetime(ufo['date'])\n",
    "print(ufo['date'].dtype)\n",
    "ufo.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "aa3ae45c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length_of_time    143\n",
      "state             419\n",
      "type              159\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "## Dropping missing data\n",
    "print(ufo[['length_of_time', 'state', 'type']].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b094faf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4283, 11)\n"
     ]
    }
   ],
   "source": [
    "ufo_no_missing = ufo[ufo['length_of_time'].notnull() & ufo['state'].notnull() & ufo['type'].notnull()]\n",
    "print(ufo_no_missing.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "465f13f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ufo['length_of_time'] = ufo.length_of_time.astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7556fe89",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    length_of_time  minutes\n",
      "0          2 weeks      2.0\n",
      "1           30sec.     30.0\n",
      "2              nan      NaN\n",
      "3  about 5 minutes      NaN\n",
      "4                2      2.0\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "## Extracting numbers from strings\n",
    "def return_minutes(time_string):\n",
    "    pattern = re.compile(r'\\d+')\n",
    "    num = re.match(pattern, time_string)\n",
    "    if num is not None:\n",
    "        return int(num.group(0))\n",
    "\n",
    "ufo['minutes'] = ufo['length_of_time'].apply(return_minutes)\n",
    "\n",
    "print(ufo[['length_of_time', 'minutes']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ae4fb772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seconds    3.156735e+10\n",
      "minutes    8.709933e+02\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "## Identifying features for standardization\n",
    "print(ufo[['seconds','minutes']].var())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a6d72f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16157636558511937\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "## log normalize seconds column\n",
    "ufo['seconds_log'] = np.log(ufo['seconds'])\n",
    "print(ufo['seconds_log'].var())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "20d38876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['date', 'city', 'state', 'country', 'type', 'seconds', 'length_of_time',\n",
       "       'desc', 'recorded', 'lat',\n",
       "       ...\n",
       "       'flash', 'formation', 'light', 'other', 'oval', 'rectangle', 'sphere',\n",
       "       'teardrop', 'triangle', 'unknown'],\n",
       "      dtype='object', length=226)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Encoding categorical variables\n",
    "ufo['country_enc'] = ufo['country'].apply(lambda x: 1 if x=='us' else 0)\n",
    "print(len(ufo['type'].unique()))\n",
    "\n",
    "# Create a one-hot encoded set of the type values\n",
    "type_set = pd.get_dummies(ufo['type'])\n",
    "\n",
    "# Concatenate this set back to the ufo DataFrame\n",
    "ufo = pd.concat([ufo, type_set], axis =1)\n",
    "\n",
    "ufo.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e9d63962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    date  month  year\n",
      "0    2011-11-03 19:21:00     11  2011\n",
      "1    2004-10-03 19:05:00     10  2004\n",
      "2    2009-09-25 21:00:00      9  2009\n",
      "3    2002-11-21 05:45:00     11  2002\n",
      "4    2010-08-19 12:55:00      8  2010\n",
      "...                  ...    ...   ...\n",
      "4930 2000-07-05 19:30:00      7  2000\n",
      "4931 2008-03-18 22:00:00      3  2008\n",
      "4932 2005-06-15 02:30:00      6  2005\n",
      "4933 1991-11-01 03:00:00     11  1991\n",
      "4934 2005-12-10 18:00:00     12  2005\n",
      "\n",
      "[4935 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "## Features from dates\n",
    "\n",
    "ufo['month'] = ufo['date'].apply(lambda x: x.month)\n",
    "ufo['year'] = ufo['date'].apply(lambda x: x.year)\n",
    "\n",
    "print(ufo[['date','month','year']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "32baa09c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Red blinking objects similar to airplanes or s...\n",
       "1                 Many fighter jets flying towards UFO\n",
       "2    Green&#44 red&#44 and blue pulses of light tha...\n",
       "3    It was a large&#44 triangular shaped flying ob...\n",
       "4       A white spinning disc in the shape of an oval.\n",
       "Name: desc, dtype: object"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ufo['desc'] = ufo.desc.astype('str')\n",
    "ufo['desc'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "38e8f7ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4935, 6434)\n"
     ]
    }
   ],
   "source": [
    "## Text Vectorization\n",
    "vec = TfidfVectorizer()\n",
    "desc_tfidf = vec.fit_transform(ufo['desc'])\n",
    "print(desc_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a3b94b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             seconds_log   seconds   minutes\n",
      "seconds_log          1.0  1.000000       NaN\n",
      "seconds              1.0  1.000000 -0.147198\n",
      "minutes              NaN -0.147198  1.000000\n"
     ]
    }
   ],
   "source": [
    "# Check the correlation between the seconds, seconds_log, and minutes columns\n",
    "print(ufo[['seconds_log','seconds','minutes']].corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "4552e40f",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "599",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[105], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m X \u001b[38;5;241m=\u001b[39m ufo\u001b[38;5;241m.\u001b[39mdrop(to_drop, axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Let's also filter some words out of the text vector we created\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m filtered_words \u001b[38;5;241m=\u001b[39m words_to_filter(vocab, vec\u001b[38;5;241m.\u001b[39mvocabulary_,desc_tfidf,\u001b[38;5;241m4\u001b[39m)\n",
      "Cell \u001b[1;32mIn[40], line 6\u001b[0m, in \u001b[0;36mwords_to_filter\u001b[1;34m(vocab, original_vocab, vector, top_n)\u001b[0m\n\u001b[0;32m      2\u001b[0m filter_list \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, vector\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]):\n\u001b[0;32m      4\u001b[0m  \n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# Here we'll call the function from the previous exercise, and extend the list we're creating\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m     filtered \u001b[38;5;241m=\u001b[39m return_weights(vocab, original_vocab, vector, i, top_n)\n\u001b[0;32m      7\u001b[0m     filter_list\u001b[38;5;241m.\u001b[39mextend(filtered)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Return the list in a set, so we don't get duplicate word indices\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[39], line 7\u001b[0m, in \u001b[0;36mreturn_weights\u001b[1;34m(vocab, original_vocab, vector, vector_index, top_n)\u001b[0m\n\u001b[0;32m      4\u001b[0m zipped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(vector[vector_index]\u001b[38;5;241m.\u001b[39mindices, vector[vector_index]\u001b[38;5;241m.\u001b[39mdata))\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Let's transform that zipped dict into a series\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m zipped_series \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mSeries({vocab[i]:zipped[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m vector[vector_index]\u001b[38;5;241m.\u001b[39mindices})\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Let's sort the series to pull out the top n weighted words\u001b[39;00m\n\u001b[0;32m     10\u001b[0m zipped_index \u001b[38;5;241m=\u001b[39m zipped_series\u001b[38;5;241m.\u001b[39msort_values(ascending \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m)[:top_n]\u001b[38;5;241m.\u001b[39mindex\n",
      "Cell \u001b[1;32mIn[39], line 7\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      4\u001b[0m zipped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(vector[vector_index]\u001b[38;5;241m.\u001b[39mindices, vector[vector_index]\u001b[38;5;241m.\u001b[39mdata))\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Let's transform that zipped dict into a series\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m zipped_series \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mSeries({vocab[i]:zipped[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m vector[vector_index]\u001b[38;5;241m.\u001b[39mindices})\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Let's sort the series to pull out the top n weighted words\u001b[39;00m\n\u001b[0;32m     10\u001b[0m zipped_index \u001b[38;5;241m=\u001b[39m zipped_series\u001b[38;5;241m.\u001b[39msort_values(ascending \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m)[:top_n]\u001b[38;5;241m.\u001b[39mindex\n",
      "\u001b[1;31mKeyError\u001b[0m: 599"
     ]
    }
   ],
   "source": [
    "## make a list of columns to drop\n",
    "to_drop = ['city', 'country', 'date', 'desc', 'lat', 'length_of_time', 'long', 'minutes', 'recorded', 'seconds', 'state']\n",
    "X = ufo.drop(to_drop, axis = 1)\n",
    "# Let's also filter some words out of the text vector we created\n",
    "filtered_words = words_to_filter(vocab, vec.vocabulary_,desc_tfidf,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f7a51c72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['type', 'seconds_log', 'country_enc', 'changing', 'chevron', 'cigar',\n",
      "       'circle', 'cone', 'cross', 'cylinder',\n",
      "       ...\n",
      "       'flash', 'formation', 'light', 'other', 'oval', 'rectangle', 'sphere',\n",
      "       'teardrop', 'triangle', 'unknown'],\n",
      "      dtype='object', length=215)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [4935, 11]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[107], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(X\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Split the X and y sets using train_test_split, setting stratify=y\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m train_X, test_X, train_y, test_y \u001b[38;5;241m=\u001b[39m train_test_split(X, y, stratify \u001b[38;5;241m=\u001b[39m y)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:2559\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   2556\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_arrays \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   2557\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAt least one array required as input\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 2559\u001b[0m arrays \u001b[38;5;241m=\u001b[39m indexable(\u001b[38;5;241m*\u001b[39marrays)\n\u001b[0;32m   2561\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m _num_samples(arrays[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m   2562\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m _validate_shuffle_split(\n\u001b[0;32m   2563\u001b[0m     n_samples, test_size, train_size, default_test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.25\u001b[39m\n\u001b[0;32m   2564\u001b[0m )\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:443\u001b[0m, in \u001b[0;36mindexable\u001b[1;34m(*iterables)\u001b[0m\n\u001b[0;32m    424\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Make arrays indexable for cross-validation.\u001b[39;00m\n\u001b[0;32m    425\u001b[0m \n\u001b[0;32m    426\u001b[0m \u001b[38;5;124;03mChecks consistent length, passes through None, and ensures that everything\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    439\u001b[0m \u001b[38;5;124;03m    sparse matrix, or dataframe) or `None`.\u001b[39;00m\n\u001b[0;32m    440\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    442\u001b[0m result \u001b[38;5;241m=\u001b[39m [_make_indexable(X) \u001b[38;5;28;01mfor\u001b[39;00m X \u001b[38;5;129;01min\u001b[39;00m iterables]\n\u001b[1;32m--> 443\u001b[0m check_consistent_length(\u001b[38;5;241m*\u001b[39mresult)\n\u001b[0;32m    444\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:397\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    395\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[0;32m    396\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 397\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    398\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    399\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[0;32m    400\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [4935, 11]"
     ]
    }
   ],
   "source": [
    "## Modeling UFO dataset: part 1\n",
    "y = 'country_enc'\n",
    "print(X.columns)\n",
    "\n",
    "# Split the X and y sets using train_test_split, setting stratify=y\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "ba6718fa",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'filtered_words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[108], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Use the list of filtered words we created to filter the text vector\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m filtered_text \u001b[38;5;241m=\u001b[39m desc_tfidf[:, \u001b[38;5;28mlist\u001b[39m(filtered_words)]\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Split the X and y sets using train_test_split, setting stratify=y \u001b[39;00m\n\u001b[0;32m      5\u001b[0m train_X, test_X, train_y, test_y \u001b[38;5;241m=\u001b[39m train_test_split(filtered_text\u001b[38;5;241m.\u001b[39mtoarray(), y, stratify\u001b[38;5;241m=\u001b[39my)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'filtered_words' is not defined"
     ]
    }
   ],
   "source": [
    "# Use the list of filtered words we created to filter the text vector\n",
    "filtered_text = desc_tfidf[:, list(filtered_words)]\n",
    " \n",
    "# Split the X and y sets using train_test_split, setting stratify=y \n",
    "train_X, test_X, train_y, test_y = train_test_split(filtered_text.toarray(), y, stratify=y)\n",
    " \n",
    "# Fit nb to the training sets\n",
    "nb.fit(train_X,train_y)\n",
    " \n",
    "# Print the score of nb on the test sets\n",
    "print(nb.score(test_X,test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7f91a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
